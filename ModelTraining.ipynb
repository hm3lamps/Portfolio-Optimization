{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hardik\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2802: DtypeWarning: Columns (27,28,33,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date, time, timedelta\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from math import *\n",
    "import calendar\n",
    "import pickle\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import History \n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "def columnToFloat(table,colname):\n",
    "    #This function will take a column in a data frame that has numbers stored as strings and convert them to floats\n",
    "    sample_name = list(table[colname])\n",
    "    sample_name = list(map(float, sample_name))\n",
    "    return sample_name\n",
    "\n",
    "def add_months(sourcedate,months):\n",
    "    month = sourcedate.month - 1 + months\n",
    "    year = sourcedate.year + month // 12\n",
    "    month = month % 12 + 1\n",
    "    day = min(sourcedate.day,calendar.monthrange(year,month)[1])\n",
    "    return date(year,month,day)\n",
    "\n",
    "def getFactordata():\n",
    "    #Read in the total_data5 set, IT's MASSIVE\n",
    "    totalset = pd.read_csv('./factor_data/Total_Data5.csv')\n",
    "    totalset = totalset[['DATE','TICKER','mcap','EP','BP','CP','SP','ES', 'CRET','RET','REP','RBP','RCP','RSP','CTEF','PM1', 'BR2','VOL','TOT','MRV1']]\n",
    "    totalset = totalset.drop_duplicates(subset = ['DATE','TICKER'],keep='first')\n",
    "    #Remove missing values from data\n",
    "    totalset = totalset[totalset['ES'] != '.']\n",
    "    totalset = totalset[totalset['CTEF'] != '.']\n",
    "    totalset = totalset[totalset['PM1'] != '.']\n",
    "    #RET is in percent, put in decimal\n",
    "    totalset['RET'] = totalset['RET']/100\n",
    "    totalset = totalset.reset_index()\n",
    "    totalset = totalset.iloc[:,1:]\n",
    "    totalset['ES'] = columnToFloat(totalset,'ES')\n",
    "    totalset['CTEF'] = columnToFloat(totalset,'CTEF')\n",
    "    totalset['PM1'] = columnToFloat(totalset,'PM1')\n",
    "    totalset = totalset.replace('.', np.NaN)\n",
    "    totalset = totalset.ffill()\n",
    "    totalset = totalset.dropna()\n",
    "    totalset = totalset.dropna()\n",
    "    return totalset\n",
    "    \n",
    "def filterUnivTotalSet(totalset,DATE):\n",
    "\n",
    "    #Important for part 3\n",
    "    totalset_sub = totalset[totalset['DATE'] == DATE] #whatever format its in\n",
    "    totalset_sub = totalset_sub.reset_index()\n",
    "    totalset_sub = totalset_sub.iloc[:,1:]\n",
    "    \n",
    "    #PART 2 - A\n",
    "    #Rank by mcap, then collect top 4000\n",
    "    totalset_sub = totalset_sub.sort_values(['mcap'], ascending=False)\n",
    "    totalset_sub = totalset_sub.iloc[:4000,:] \n",
    "    \n",
    "    #PART 2 - B\n",
    "    #Eli Schwartz\n",
    "    #Choose 70th percentile and above for ES value from totalset_sub\n",
    "    totalset_sub = totalset_sub.sort_values(['ES'], ascending=False)\n",
    "    \n",
    "    n=len(totalset_sub)\n",
    "    index= int(n*7/10) #70th percentile\n",
    "    totalset_sub = totalset_sub.iloc[index:,:]\n",
    "    \n",
    "    #Reset index\n",
    "    totalset_sub = totalset_sub.reset_index()\n",
    "    totalset_sub = totalset_sub.iloc[:,1:]\n",
    "    \n",
    "    return totalset_sub\n",
    "\n",
    "def subtractYear(dateCons, year):\n",
    "    year = dateCons.year - year\n",
    "    month = dateCons.month\n",
    "    day =  dateCons.day\n",
    "    dateString = str(month) + \"/\" + str(day) + \"/\" + str(year)\n",
    "    yearBackDate = datetime.strptime(dateString, '%m/%d/%Y')\n",
    "    return yearBackDate\n",
    "\n",
    "def splitTrainTest(dataset, testSize):\n",
    "    y = dataset.iloc[:, 0:1]\n",
    "    data = dataset.iloc[:, 1:19]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.4)\n",
    "    return  X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def getTrainingFrame(dateCons, totalset, tickList10Fac):\n",
    "    datePrevYear = subtractYear(dateCons, 25)\n",
    "    datePrevYear = add_months(datePrevYear, 1)\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(300):\n",
    "        dateNext = add_months(datePrevYear, 1)\n",
    "        retNextPeriod = totalset[totalset['DATE'] == dateNext]\n",
    "        smallFil = totalset[totalset['DATE'] == datePrevYear] \n",
    "        retNextPeriod = retNextPeriod[['TICKER', 'CRET']]\n",
    "        retNextPeriod = retNextPeriod.loc[retNextPeriod.TICKER.isin(tickList10Fac)]\n",
    "        factorList = ['TICKER','mcap','EP','BP','CP','SP','ES','RET','REP','RBP','RCP','RSP','CTEF','PM1', 'BR2','VOL','TOT','MRV1']\n",
    "        smallFil =  smallFil[factorList]\n",
    "        smallFil =  smallFil.loc[smallFil.TICKER.isin(tickList10Fac)]\n",
    "        df = df.append(pd.merge(retNextPeriod, smallFil, on='TICKER'))\n",
    "        datePrevYear = dateNext\n",
    "    del df['TICKER']\n",
    "    return df\n",
    "\n",
    "def KNNPredictionUsingOptimalK(df):    \n",
    "    X_train, X_test, y_train, y_test = splitTrainTest(df, 0.3)\n",
    "    \n",
    "    neigh = KNeighborsRegressor(n_neighbors=2, algorithm = 'auto', weights = 'uniform')\n",
    "    neigh.fit(X_train, y_train)\n",
    "    #print(neigh.predict(x))\n",
    "    TrainPred = neigh.predict(X_train)\n",
    "    TestPred = neigh.predict(X_test)\n",
    "    TrainError = np.sum((TrainPred -y_train)**2)\n",
    "    TestError = np.sum((TestPred - y_test)**2)\n",
    "    \n",
    "    NumberResultsKNN = pd.DataFrame({'K': [2], 'TrainError': [TrainError[0]],'TestError': [TestError[0]]})\n",
    "\n",
    "    for n in range(3,20,1):\n",
    "        neigh = KNeighborsRegressor(n_neighbors=n, algorithm = 'auto', weights = 'uniform')\n",
    "        neigh.fit(X_train, y_train)\n",
    "        TrainPred = neigh.predict(X_train)\n",
    "        TestPred = neigh.predict(X_test)\n",
    "        TrainError = np.sum((TrainPred -y_train)**2)\n",
    "        TestError = np.sum((TestPred - y_test)**2)\n",
    "        \n",
    "        NumberResultsKNN = NumberResultsKNN.append(pd.DataFrame({'K': [n], 'TrainError': [TrainError[0]],'TestError': [TestError[0]]}))\n",
    "    \n",
    "    NumberResultsKNN = NumberResultsKNN.reset_index()\n",
    "    NumberResultsKNN = NumberResultsKNN.iloc[:,1:]\n",
    "    plt.scatter(NumberResultsKNN['K'], NumberResultsKNN['TrainError'], c='k', label='TrainError')\n",
    "    plt.plot(NumberResultsKNN['K'], NumberResultsKNN['TestError'], c='g', label='TestError')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    BestK = NumberResultsKNN['K'][0]\n",
    "    SmallestDifference = abs(NumberResultsKNN['TrainError'][0] - NumberResultsKNN['TestError'][0])\n",
    "    for k in range(len(NumberResultsKNN)):\n",
    "        if abs(NumberResultsKNN['TrainError'][k] - NumberResultsKNN['TestError'][k]) < SmallestDifference:\n",
    "            SmallestDifference = abs(NumberResultsKNN['TrainError'][k] - NumberResultsKNN['TestError'][k])\n",
    "            BestK = NumberResultsKNN['K'][k]\n",
    "    \n",
    "    neigh = KNeighborsRegressor(n_neighbors = BestK, algorithm = 'auto', weights = 'uniform')\n",
    "    neigh.fit(X_train, y_train)\n",
    "    filename = \"./\"+ resultpath + '/KNNStockPredictor.pkl'\n",
    "    pickle.dump(neigh, open(filename, 'wb'))\n",
    "    \n",
    "    return BestK\n",
    "\n",
    "def neuralNetwork(df):\n",
    "    history = History()\n",
    "    X_train, X_test, y_train, y_test = splitTrainTest(df, 0.3)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=6, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adadelta())\n",
    "\n",
    "    feature_cols = X_train\n",
    "    labels = y_train.values\n",
    "\n",
    "    model.fit(np.array(feature_cols), np.array(labels), epochs=5, batch_size=100)\n",
    "\n",
    "    filename = \"./\"+ resultpath + '/NNStockPredictor.h5'\n",
    "    \n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"./\"+ resultpath + \"/modelNN.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(filename)\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    \n",
    "    resultpath = \"Model\"\n",
    "    if(not os.path.isdir(resultpath)):\n",
    "        os.mkdir(resultpath)\n",
    "        \n",
    "    totalset = getFactordata()\n",
    "    totalset['DATE'] = pd.to_datetime(totalset['DATE'])\n",
    "\n",
    "    dateString = '12/01/2004'\n",
    "    dateCons = datetime.strptime(dateString, '%m/%d/%Y')\n",
    "\n",
    "    filteredTotalSet = filterUnivTotalSet(totalset, dateCons)\n",
    "    filteredTotalSet['DATE'] = pd.to_datetime(filteredTotalSet['DATE'])\n",
    "    filteredTotalSet = filteredTotalSet.sort_values(by=['TICKER'])\n",
    "    tickerList = filteredTotalSet['TICKER'].values.tolist()\n",
    "    \n",
    "    df = getTrainingFrame(dateCons, totalset, tickerList)\n",
    "    df = df.convert_objects(convert_numeric=True)\n",
    "    df =  df.dropna()\n",
    "    \n",
    "    print(df.shape)\n",
    "    \n",
    "    bestK = KNNPredictionUsingOptimalK(df)\n",
    "    print(\"Our Best K found:\" , bestK )\n",
    "    \n",
    "    neuralNetwork(df)\n",
    "    print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
